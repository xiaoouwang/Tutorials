{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import torch\n",
    "from numba import cuda\n",
    "\n",
    "\n",
    "class DatasetLoader:\n",
    "    def __init__(self, ):\n",
    "        pass\n",
    "        # self.text_col = text_col\n",
    "        # self.dataframe = dataframe\n",
    "        # self.label_col = label_col\n",
    "\n",
    "    def load_dataset(self, dataframe, text_col=\"text\", label_col=\"label\"):\n",
    "        # Split the DataFrame into training and validation sets\n",
    "        train_df = self.dataframe[self.dataframe['split'] == 'train']\n",
    "        val_df = self.dataframe[self.dataframe['split'] == 'validation']\n",
    "        # get the number of unique labels\n",
    "        num_labels = len(train_df[self.label_col].unique())\n",
    "        train_dataset = Dataset.from_pandas(train_df)\n",
    "        val_dataset = Dataset.from_pandas(val_df)\n",
    "        print(f\"\\nDataset loaded. The dataset has {self.num_labels} labels, {len(train_df)} training items, {len(val_df)} validation items. \\n{self.dataframe.head(3)}\")\n",
    "        return train_dataset, val_dataset, num_labels\n",
    "\n",
    "class Classifier:\n",
    "    def __init__(self, model_name = \"camembert/camembert-base-ccnet-4gb\", nb_labels = 2):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "        self.f1 = None\n",
    "        self.accuracy = None\n",
    "        self.recall = None\n",
    "        self.nb_labels = nb_labels\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name,num_labels=self.num_labels)\n",
    "        self.model_size = self.get_model_size(self.model)[1]\n",
    "        self.model = None\n",
    "        print(f\"\\nModel loaded. We will finetune {self.model_name} with {self.nb_labels} labels.\")\n",
    "\n",
    "        # Additional configurations can be added here\n",
    "\n",
    "    def get_model_size(self,model):\n",
    "        # from https://camembert-model.fr/posts/tutorial/\n",
    "        param_size = 0\n",
    "        param_count = 0\n",
    "        for param in model.parameters():\n",
    "            param_count += param.nelement()\n",
    "            param_size += param.nelement() * param.element_size()\n",
    "        buffer_size = 0\n",
    "        for buffer in model.buffers():\n",
    "            buffer_size += buffer.nelement() * buffer.element_size()\n",
    "        size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "        return param_count, f'{size_all_mb:.2f}'\n",
    "\n",
    "    def tokenize_function(self, examples):\n",
    "        if \"camembert\" in self.model_name:\n",
    "            return self.tokenizer(examples[self.text_col], padding=\"max_length\", truncation=True, max_length=512)\n",
    "        else:\n",
    "            return self.tokenizer(examples[self.text_col], padding=\"max_length\", truncation=True)\n",
    "\n",
    "    def compute_metrics(self, pred):\n",
    "        labels = pred.label_ids\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "        accuracy = accuracy_score(labels, preds)\n",
    "        recall = recall_score(labels, preds, average='weighted')\n",
    "        f1 = f1_score(labels, preds, average='weighted')\n",
    "        self.f1 = f1\n",
    "        self.accuracy = accuracy\n",
    "        self.recall = recall\n",
    "        return {'accuracy': accuracy, 'recall': recall, 'f1': f1}\n",
    "\n",
    "    def train(self, train_dataset, validation_dataset, epochs=2, batch_size=10, learning_rate=2e-5):\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(self.model_name,num_labels=self.num_labels)\n",
    "        model.to(self.device)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./results\",\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=0.01,\n",
    "            logging_strategy=\"epoch\",\n",
    "            evaluation_strategy=\"epoch\"\n",
    "        )\n",
    "        training_args.set_save(strategy=\"epoch\")\n",
    "\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=validation_dataset,\n",
    "            compute_metrics=self.compute_metrics,\n",
    "            tokenizer=self.tokenizer\n",
    "        )\n",
    "        trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy data\n",
    "data = {\n",
    "    \"review\": [f\"This is sentence {i}\" for i in range(30)],\n",
    "    \"label\": [random.randint(0, 1) for _ in range(30)],\n",
    "    \"split\": [\"train\" if i < 25 else \"validation\" for i in range(30)]\n",
    "}\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Real data\n",
    "\n",
    "df_train = pd.read_csv(\"data/train-fr-sampled.txt\", sep=\",\")\n",
    "df_val = pd.read_csv(\"data/validation-fr-sampled.txt\", sep=\",\")\n",
    "df_train[\"split\"] = \"train\"\n",
    "df_val[\"split\"] = \"validation\"\n",
    "df = pd.concat([df_train, df_val], ignore_index=True)\n",
    "df\n",
    "\n",
    "\n",
    "print(f\"{training_time} seconds\")\n",
    "\n",
    "print(classifier.f1, classifier.accuracy, classifier.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259256b59d57435685d7d9be61bf2f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c305b683871d4e4b819f8b5e2eed4e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a pandas dataframe to log model name, f1, accuracy, recall, training time, data_size, batch size, learning rate, epochs\n",
    "\n",
    "def log_model():\n",
    "    log = pd.DataFrame(columns=[\"model_name\", \"f1\", \"accuracy\", \"recall\", \"training_time\", \"data_size\", \"batch_size\", \"learning_rate\", \"epochs\"])\n",
    "    return log\n",
    "\n",
    "# create a function to add data to the log\n",
    "def add_to_log(log, model_name, f1, accuracy, recall, training_time, data_size, batch_size, learning_rate, epochs):\n",
    "    log.loc[len(log)+1] = [model_name, f1, accuracy, recall, training_time, data_size, batch_size, learning_rate, epochs]\n",
    "    return log\n",
    "\n",
    "log = log_model()\n",
    "add_to_log(log, \"camembert/camembert-base-ccnet-4gb\", classifier.f1, classifier.accuracy, classifier.recall, training_time, len(df), 10, 2e-5, 1)\n",
    "\n",
    "for m in [\"camembert-base\",\"camembert/camembert-large\",\"camembert/camembert-base-ccnet\", \"camembert/camembert-base-ccnet-4gb\", \"camembert/camembert-base-oscar-4gb\", \"camembert/camembert-base-wikipedia-4gb\",\"flaubert/flaubert_small_cased\",\"flaubert/flaubert_base_uncased\",\"flaubert/flaubert_base_cased\",\"flaubert/flaubert_large_cased\"]:\n",
    "    # Initialize the classifier\n",
    "    epoch = 10\n",
    "    batch_size = 10\n",
    "    learning_rate = 2e-5\n",
    "    classifier = Classifier(dataframe=df, model_name=m, text_col=\"review\",label_col=\"label\")\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    classifier.train(epochs=epoch, batch_size=batch_size, learning_rate=learning_rate)\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    add_to_log(log, m, classifier.f1, classifier.accuracy, classifier.recall, training_time, len(df), 10, 2e-5, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
